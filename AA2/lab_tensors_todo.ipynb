{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python"},"language_info":{"name":"python","version":"3.9.5"},"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"},"colab":{"provenance":[{"file_id":"https://github.com/telecombcn-dl/labs-all/blob/main/labs/tensors/lab_tensors_todo.ipynb","timestamp":1663323595921}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"R-tjDhXJeixW"},"source":["# A World of Tensors and Differentiable Computing\n","Created by [Santiago Pascual](https://scholar.google.es/citations?user=7cVOyh0AAAAJ&hl=ca) ([UPC School](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) 2019).\n","\n","Updated by [Gerard I. Gállego](https://www.linkedin.com/in/gerard-gallego/) and [José A. R. Fonollosa](https://www.kaggle.com/jarfo1)\n","\n","In these lab exercises we are going to see: \n","\n","1. What are tensors, concretely in the PyTorch framework.\n","2. How to operate with them, and typical operations for deep learning modeling.\n","3. Broadcasting Semantics. Added by [Jose A. R. Fonollosa](https://www.kaggle.com/jarfo1) "]},{"cell_type":"markdown","metadata":{"id":"NwQSQBAt5WL2"},"source":["## What is a Tensor?\n","\n","\n","A Tensor is the generalization of a vector into k dimensions.\n","\n","![](https://miro.medium.com/max/644/1*SGqhI_WpSaEr17wo8ycUhg.png)\n","\n","Table taken from [1].\n","\n","Because of this, a tensor is any k-dimensional structure, including matrices, vectors and scalars. PyTorch is a deep learning framework (https://pytorch.org) widely used for both research and production. As in any other deep learning framework, its core data structure is the tensor."]},{"cell_type":"code","metadata":{"id":"IpaFq0q3z03l","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1663323822100,"user_tz":-120,"elapsed":3299,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"15e8d2df-4acd-4c21-a6d8-a2ef94225894"},"source":["# We first import PyTorch and Numpy libraries as fundamental tools to work with arrays and tensors\n","import torch\n","import numpy\n","# initialize a random seed such that every execution will raise same random sequences of results\n","torch.manual_seed(1)"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f4b6938d7d0>"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"MQQFFjMQPT1n"},"source":["#### Creating tensors with PyTorch"]},{"cell_type":"code","metadata":{"id":"SQvEkhLIz03p","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1663323837144,"user_tz":-120,"elapsed":273,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"c212c4df-2ac7-4a92-b34d-bf307bb50551"},"source":["# We can initialize an empty structure with certain dimensions:\n","a = torch.empty(5, 7)\n","# and we can check its dimensionality with the .shape attribute or .size() function\n","print(a.shape)\n","print(a.size())"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 7])\n","torch.Size([5, 7])\n"]}]},{"cell_type":"markdown","metadata":{"id":"zl28ffg_67z8"},"source":["Dimensions in PyTorch tensors are indexed from 0 onwards, so the first axis of size 5 is the *dim=0*."]},{"cell_type":"code","metadata":{"id":"8Btd1zPuz03s","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1663323861711,"user_tz":-120,"elapsed":282,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"716e5be5-fd01-47ac-bcc3-c74a8848d530"},"source":["# YAYY we have created a tensor of size 5x7, but what does it contain?\n","print(a)\n","# Rubbish, nonsense, random stuff, it could be zero, it could be nan it could be whatever"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[2.3052e-35, 0.0000e+00, 2.3262e-43, 0.0000e+00,        nan, 0.0000e+00,\n","         7.5656e+28],\n","        [1.8465e+25, 1.1431e+27, 6.9983e+28, 1.9349e-19, 4.5445e+30, 4.7429e+30,\n","         4.7567e+30],\n","        [9.2860e-04, 1.1867e+27, 3.2483e+33, 1.9690e-19, 6.8589e+22, 1.3340e+31,\n","         1.1708e-19],\n","        [1.8318e+25, 7.1125e-04, 1.1434e+27, 7.5555e+31, 1.2705e+31, 7.0948e+22,\n","         3.8946e+21],\n","        [4.4650e+30, 1.1286e+27, 2.8405e+20, 3.2608e-12, 1.4583e-19, 2.0704e-19,\n","         3.0881e+29]])\n"]}]},{"cell_type":"code","metadata":{"id":"QpQqCz6Lz03u","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1663323876557,"user_tz":-120,"elapsed":272,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"1da4d6f3-886a-4704-f9ae-af991eceb8d8"},"source":["# We can easily fill the tensor with some fixed value with the .fill_(val) function\n","a.fill_(10)\n","print(a)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[10., 10., 10., 10., 10., 10., 10.],\n","        [10., 10., 10., 10., 10., 10., 10.],\n","        [10., 10., 10., 10., 10., 10., 10.],\n","        [10., 10., 10., 10., 10., 10., 10.],\n","        [10., 10., 10., 10., 10., 10., 10.]])\n"]}]},{"cell_type":"code","metadata":{"id":"06PYDmMGz03w","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1663323886419,"user_tz":-120,"elapsed":246,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"a9201da6-49f8-4dad-a27d-02c09c30c72c"},"source":["# Of course we could go k-dimensional, we just have to put more numbers in the init function\n","a = torch.empty(2,4,6,8)\n","print(a.shape)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 4, 6, 8])\n"]}]},{"cell_type":"markdown","metadata":{"id":"_F0pCzMXkEU6"},"source":["#### There are functions in PyTorch to initialize some special tensors:\n","\n","* **torch.randn** samples from a Gaussian distribution (mean=0, std=1)\n","* **torch.rand** samples from a uniform distribution [0, 1)\n","* **torch.ones** creates a tensor with 1s\n","* **torch.zeros** creates a tensor with 0s"]},{"cell_type":"markdown","metadata":{"id":"kC_o44hLkaqy"},"source":["### Exercise 1\n","\n","Create a tensor *z* drawn from a Gaussian distribution of dimensions (16, 1024)"]},{"cell_type":"code","metadata":{"id":"oxhbwcUmz030","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1663324059221,"user_tz":-120,"elapsed":248,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"aa53c321-7599-446f-af1e-fd7a95afc676"},"source":["# TODO: Gaussian tensor 16x1024\n","z = torch.randn(16, 1024)\n","print(z)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.7310,  0.3308, -0.2276,  ...,  0.7264, -0.2090,  0.5279],\n","        [-1.4575, -0.8948,  0.7600,  ...,  1.8988,  0.6308, -1.6087],\n","        [-0.1079,  1.4335, -0.4152,  ...,  0.4991, -0.3876, -1.1899],\n","        ...,\n","        [ 0.9964, -0.4806, -0.5368,  ...,  0.0457,  0.5390, -0.7158],\n","        [-0.1110, -2.3468,  0.6205,  ..., -0.4044,  0.1216, -0.6174],\n","        [ 1.1102,  1.6454, -0.7639,  ..., -0.4366,  0.2246,  0.5743]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"xdGiSnOKmSyv"},"source":["#### Tensors data type\n","\n","Importantly, tensors have a data type (like numeric variables are int, float, double, etc.). We can check the type\n","with the **tensor.dtype** attribute. We can also change the dtype of our tensor with a very simple cast following the data type name in the form of a function: **tensor.float()**, **tensor.int()**, **tensor.long()**, etc."]},{"cell_type":"code","metadata":{"id":"nxXQ8hbXz032","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1663324094605,"user_tz":-120,"elapsed":244,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"5aa31d29-a705-4bd1-a09b-9720d172663f"},"source":["a = torch.ones(5)\n","print(a.dtype)\n","\n","# change to float64 (aka. double)\n","print(a.double().dtype)\n","\n","# change to float16 (aka. half)\n","print(a.half().dtype)\n","\n","# change to int16 (aka. short)\n","print(a.short().dtype)\n","\n","# change to int64 (aka. long)\n","print(a.long().dtype)\n"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.float32\n","torch.float64\n","torch.float16\n","torch.int16\n","torch.int64\n"]}]},{"cell_type":"markdown","metadata":{"id":"PQqUN7JGnhYl"},"source":["And the way to create a tensor with a specific data type at initialization is either by specifying the **dtype=torch.<dtype\\>** during the tensor initialization, or using an explicit tensor constructor like **torch.FloatTensor()**, **torch.LongTensor()**, etc."]},{"cell_type":"code","metadata":{"id":"2Pqp8ptIz034","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1663324140124,"user_tz":-120,"elapsed":233,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"679bbfdf-a6aa-4d05-b36f-ff9ce6d7c003"},"source":["# Initialize a tensor with type short()\n","a = torch.empty(5, 7, dtype=torch.short)\n","print(a.dtype)\n","\n","# Directly create a short tensor\n","a = torch.ShortTensor(5, 7)\n","print(a.dtype)\n","\n","# Remember: there should be rubbish in these results, we just explicited a data type, not any value yet! (hence random memory is depicted)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.int16\n","torch.int16\n"]}]},{"cell_type":"markdown","metadata":{"id":"0Tw0oG0HsXOf"},"source":["#### Tips about tensor data types applied to deep learning:\n","\n","Keep in mind the following relations, they might be very useful for your future selves!\n","\n","* Float32 --> Data type for the neural network parameters and GPU operations!\n","* Long (Int64) --> Data type for text inputs (e.g. indexes of words in a dictionary)\n","* Float16 (Half) --> Data type for currently fastest GPU operations (with less precision) on advanced GPU implementations.\n","\n","Remember: only a sticknote for your future selves, in case you have to deal with any of the above mentioned things (embeddings, fastest GPU stuff, etc.).\n","\n","**TO SEE THE FULL SET OF PYTORCH TENSOR DATA TYPES, CHECK THE DOCUMENTATION AT https://pytorch.org/docs/stable/tensors.html**"]},{"cell_type":"markdown","metadata":{"id":"qK7tJePCnIoC"},"source":["#### Bringing the tensors from Python and Numpy\n","\n","You may be familiarized with Numpy and Python lists. The former one is a MUST to do any scientific programming in Python, so if you need a refresh it is recommended to have a quick review: https://becominghuman.ai/an-essential-guide-to-numpy-for-machine-learning-in-python-5615e1758301 . The latter, lists, are the inherent mechanism of Python to create a sorted structure of elements (like a k-dimensional array, as we can embed lists in lists and so on).\n","\n","PyTorch is very well integrated with Numpy (actually PyTorch is supposed to be an enhanced Numpy, with algebraic operations also running on GPU!) and Python. We can hence convert our Numpy and lists into PyTorch tensors VERY EASILY!"]},{"cell_type":"code","metadata":{"id":"NFF57Py-z038","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1663324395002,"user_tz":-120,"elapsed":315,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"35f37f08-1717-4a96-f81d-8e6fa86bf2e9"},"source":["# Creating a 1-D tensor from the Numpy array [1, 2, 3]\n","a = torch.tensor(numpy.array([1, 2, 3]))\n","\n","# Creating a 1-D tensor from the Python list [1, 2, 3]\n","a = torch.tensor([1, 2, 3])\n","# Values 1, 2, 3\n","print('Tensor a values: ', a)\n","# 1 dimension of size 3\n","print('Tensor a shape: ', a.shape)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor a values:  tensor([1, 2, 3])\n","Tensor a shape:  torch.Size([3])\n"]}]},{"cell_type":"code","metadata":{"id":"sm21KH-oz039","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1663324411180,"user_tz":-120,"elapsed":292,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"d83daa1f-9461-4a56-9270-03103aacc74e"},"source":["# k-dimensional arrays are also turned into PyTorch tensors as easily as that\n","A = torch.tensor(numpy.ones((16, 1024)))\n","print(A.dtype)\n","print(A.shape)"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.float64\n","torch.Size([16, 1024])\n"]}]},{"cell_type":"markdown","metadata":{"id":"-J5CGBU3V5gF"},"source":["#### Converting tensors back to Numpy!\n"," \n","Converting back to numpy arrays is as easy as getting the *.data* attribute of the tensor and calling its *.numpy()* casting function\n"]},{"cell_type":"code","metadata":{"id":"VhEdKeEhz03_","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1663324433448,"user_tz":-120,"elapsed":312,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"81a61d6e-25d7-4dbd-8706-6e9eb696e739"},"source":["A = torch.rand(10, 10)\n","Anpy = A.data.numpy()\n","print('A type: ', type(A)) # torch.Tensor\n","print('Anpy type: ', type(Anpy)) # numpy.ndarray"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["A type:  <class 'torch.Tensor'>\n","Anpy type:  <class 'numpy.ndarray'>\n"]}]},{"cell_type":"markdown","metadata":{"id":"paUskvqytogM"},"source":["### Exercise 2\n","\n","Create an **int16** *it* tensor in PyTorch (however you want) from the following numpy array *na*"]},{"cell_type":"code","metadata":{"id":"etgT7qPlz04A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663328955364,"user_tz":-120,"elapsed":270,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"476619c5-8b13-4bb1-edcc-c28d28ee848f"},"source":["na = 10 * numpy.random.rand(8, 8)\n","\n","# TODO: create the short tensor out of 'na'\n","it = torch.ShortTensor(na)\n","print(it)"],"execution_count":90,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[5, 6, 6, 5, 1, 1, 6, 3],\n","        [3, 9, 7, 2, 3, 5, 1, 2],\n","        [3, 1, 3, 0, 1, 9, 5, 3],\n","        [0, 3, 9, 4, 2, 9, 6, 0],\n","        [6, 9, 9, 4, 3, 5, 4, 6],\n","        [9, 9, 6, 2, 4, 7, 9, 6],\n","        [3, 0, 4, 6, 8, 3, 8, 3],\n","        [5, 6, 5, 1, 6, 9, 7, 6]], dtype=torch.int16)\n"]}]},{"cell_type":"markdown","metadata":{"id":"GHyxb3EkxHaT"},"source":["## Operations with tensors\n","\n","The documentation of PyTorch tensors can be found online in: https://pytorch.org/docs/stable/tensors.html \n","\n","This section introduces two important types of operations in PyTorch:\n","\n","1. In-place operations\n","2. Algebraic operations on tensors: transposing, squeezing/unsqueezing, slicing, chunking and concatenating"]},{"cell_type":"markdown","metadata":{"id":"ybnrLcAlNeJV"},"source":["#### In-place operations"]},{"cell_type":"code","metadata":{"id":"ixmzU3Jiz04E","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1663324706033,"user_tz":-120,"elapsed":337,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"c97a1b4c-0840-4602-a271-716e90fde7f6"},"source":["# In-place operations are those whose function name contain an underscore '_' as in fill_(val), add_(val), etc. \n","a = torch.empty(2, 2)\n","a.fill_(1)\n","print(a)\n"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1.],\n","        [1., 1.]])\n"]}]},{"cell_type":"code","metadata":{"id":"fSXcu30fz04F","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1663324707134,"user_tz":-120,"elapsed":5,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"f44763f9-9e54-41d5-9a8c-a1d245500395"},"source":["# There are operations where both inplace and normal methods can be applied\n","# For example to sum some value to the tensor\n","a.add_(1)\n","print(a) # prints a tensor of values \"2\"\n","# This, though, takes no effect\n","a.add(1)\n","print(a)"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[2., 2.],\n","        [2., 2.]])\n","tensor([[2., 2.],\n","        [2., 2.]])\n"]}]},{"cell_type":"code","metadata":{"id":"dj7-6EMvz04F","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1663324722788,"user_tz":-120,"elapsed":214,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"80b40069-cdbf-4e2f-a72e-11676d4fdcc0"},"source":["# So yes, you guessed right! We have to actually assign the result to an output tensor to actually \n","# get the outcome of this operation\n","b = a.add(1)\n","print(b) # NOW it prints a tensor of values \"3\"!"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[3., 3.],\n","        [3., 3.]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"YwDuaN_w8ZQ3"},"source":["### Exercise 3\n","\n","Do you notice an important difference between these in-place vs normal operations? Perhaps not yet... what if I tell you that I want to apply an operation upon a FloatTensor 10000x10000? Knowing that we have 32 bits per float value, compute the required memory to store that tensor in Megabytes (1 MB = 1.000.000 Bytes)"]},{"cell_type":"code","metadata":{"id":"m_Z9_or9z04G","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1663324801005,"user_tz":-120,"elapsed":354,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"34341238-35da-46c4-c328-7f1457fce17b"},"source":["# TODO\n","total_mem = 10000*10000*32/1000000\n","\n","print(total_mem)"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["3200.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"OGXDnygK-adC"},"source":["### Now we can be more applied to deep learning. But with tensors, raw operations. \n","\n","A Neuron is defined as a linear operation of weighted sums followed by a non-linearity. We thus have a tensor of weights *w*, a scalar with the bias *b*, and a non-linearity (like ReLU *max(0, x)* that just allows the positive components to go forth in the *y* values).\n","\n","![](https://www.researchgate.net/profile/Haroldo_Campos_Velho2/publication/235901708/figure/fig1/AS:669443441049602@1536619162135/Artificial-neuron-Equation-neuron-output.ppm)\n"]},{"cell_type":"code","metadata":{"id":"zCxIB0HZz04H","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1663325145548,"user_tz":-120,"elapsed":313,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"6b0bf7d4-acf5-4cfd-9e71-f4be5caa2f06"},"source":["# We will have 1 input vector with 100.000 dimensions (features)\n","x = torch.ones(1, 100000)\n","\n","# Our weight tensor is hence, for a neuron, 100.000 x 1\n","w = 0.02 * torch.randn(100000, 1)\n","\n","\n","# Let's define the function that will perform the operation of a neuron\n","def forward_neuron(x, w, b):\n","  v = x.mm(w) + b # .mm() is the matmul function (http://pytorch.org/docs/stable/torch.html#torch.mm)\n","  y = v.clamp(min=0) # relu is defined as a truncation of the negative activations to zero (clamp function does the trick)\n","  return y\n","\n","# Now we can see examples of operation through the relu\n","\n","# Our bias is just a scalar\n","bp = 10 * torch.ones(1)\n","print(forward_neuron(x, w, bp))\n","\n","# Shifting the bias quite negatively should raise zero\n","bn = -10 * torch.ones(1)\n","print(forward_neuron(x, w, bn))"],"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[12.2880]])\n","tensor([[12.2880]])\n","tensor([[12.2880]])\n","tensor([[-7.7120]])\n","tensor([[0.]])\n","tensor([[0.]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"qWXKiv2-xTWF"},"source":["### Exercise 4\n","\n","Continuing with previous in-place vs normal operations rationale, please change the *forward_neuron* function to apply the ReLU in-place. This is very useful to save memory when constructing very deep nets. \n","\n","\n","\n","**NOTE:** for the record, this will solve the doubt you will have some day \"what is this inplace=True in the *nn.ReLU(inplace=True)* object?\" when you build neural networks with the PyTorch *torch.nn* API."]},{"cell_type":"code","metadata":{"id":"yugi0mqUz04I","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1663325964545,"user_tz":-120,"elapsed":313,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"f8901023-e1a3-44fa-8376-0e6c08baf4cf"},"source":["def forward_neuron(x, w, b):\n","  v = x.mm(w) + b # .mm() is the matmul function (http://pytorch.org/docs/stable/torch.html#torch.mm)\n","  # TODO: make the inplace clamping\n","  # if v < 0:\n","  #   v.fill_(0)\n","  v.clamp_(min=0)\n","  \n","  return v\n","\n","print(forward_neuron(x, w, bn))"],"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"Y85-XopvNafw"},"source":["#### Transpositions and beyond\n","\n","Bear in mind the following FUNDAMENTAL operations to work with deep learning:\n","\n","* Tensor transposition: swapping dimensions in the tensor\n","* Tensor chunking: breaking down a tensor into sub-pieces through a certain dimension\n","* Tensor concatenation: merging different tensors into a single one.\n","* Tensor squeezing/unsqueezing for dimension adjustments"]},{"cell_type":"code","metadata":{"id":"MnzfrHE0z04J","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1663326483774,"user_tz":-120,"elapsed":259,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"b9cf7ad6-f5fa-4af5-e11c-59c5b63e4c76"},"source":["# Transposition\n","\n","A = torch.empty(10, 20, 5)\n","\n","# Swap axis 2 and 1\n","A_21 = A.transpose(2, 1)\n","A_12 = A.transpose(1, 2)\n","print('{} transposed axis (2, 1) to: {}'.format(A.shape, A_21.shape))\n","print('{} transposed axis (1, 2) to: {}'.format(A.shape, A_12.shape))\n","\n","# Swap axis 2 and 0\n","A_20 = A.transpose(2, 0)\n","\n","print('{} transposed axis (2, 0) to: {}'.format(A.shape, A_20.shape))"],"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 20, 5]) transposed axis (2, 1) to: torch.Size([10, 5, 20])\n","torch.Size([10, 20, 5]) transposed axis (1, 2) to: torch.Size([10, 5, 20])\n","torch.Size([10, 20, 5]) transposed axis (2, 0) to: torch.Size([5, 20, 10])\n"]}]},{"cell_type":"markdown","metadata":{"id":"YiCQwQyTRBiu"},"source":["**NOTE:** The utlity of transpositions will be seen further when dealing with different neural architecture designs."]},{"cell_type":"code","metadata":{"id":"HxXwrR_nz04K","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1663326493877,"user_tz":-120,"elapsed":284,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"05a614dc-3075-493a-c5ab-f77f73163bbf"},"source":["# Different axis can be merged with the .view() operator\n","B = A.view(200, 5)\n","print('{} axis (0, 1) merged to: {}'.format(A.shape, B.shape))"],"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 20, 5]) axis (0, 1) merged to: torch.Size([200, 5])\n"]}]},{"cell_type":"code","metadata":{"id":"IPXvTDjGz04L","executionInfo":{"status":"ok","timestamp":1663326490474,"user_tz":-120,"elapsed":248,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}}},"source":["# Note that giving a wrong size in the dimensions for .view() raises an error\n","try:\n","  B = A.view(50, 20)\n","except RuntimeError:\n","  print('Wrong dimension sizes specified in .view()!')  "],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"id":"GqyGgkTIz04L","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1663326101644,"user_tz":-120,"elapsed":258,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"87a2948d-58f0-4e30-8e60-3d764f060459"},"source":["# Chunking the tensor with .chunk() requires to specify how many chunks we want in which dimension\n","# For example for tensor A: (10, 20, 5), we can chunk it into 5 sub-tensors of shape (10, 4, 5) each\n","Achunks = torch.chunk(A, 5, dim=1)\n","for i, achunk in enumerate(Achunks):\n","  print('Chunk {} shape: {}'.format(i, achunk.shape))"],"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["Chunk 0 shape: torch.Size([10, 4, 5])\n","Chunk 1 shape: torch.Size([10, 4, 5])\n","Chunk 2 shape: torch.Size([10, 4, 5])\n","Chunk 3 shape: torch.Size([10, 4, 5])\n","Chunk 4 shape: torch.Size([10, 4, 5])\n"]}]},{"cell_type":"code","metadata":{"id":"TGhe1qVfz04M","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1663326170058,"user_tz":-120,"elapsed":346,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"21783d5e-305d-4ef6-97d9-31f23fe291a1"},"source":["# And tensors can be merged back to a tensor Amerged with .cat() operator, specifying in which dimension do we concatenate\n","# So to go back to the same tensor as we had prior to chunking, we stack on dimension 1\n","\n","Amerged = torch.cat(Achunks, dim=1)\n","print('Amerged shape: ', Amerged.shape)"],"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["Amerged shape:  torch.Size([10, 20, 5])\n"]}]},{"cell_type":"markdown","metadata":{"id":"sIIVSC4Ez7VF"},"source":["Finally, we may want to add additional dimensions or remove them from our tensor.\n","We achieve so with [.squeeze()](https://pytorch.org/docs/stable/torch.html#torch.squeeze) or [.unsqueeze()](https://pytorch.org/docs/stable/torch.html#torch.unsqueeze).\n","\n"]},{"cell_type":"code","metadata":{"id":"735hSa-cz04N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663327282423,"user_tz":-120,"elapsed":262,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"848c0399-3677-48bb-d502-970f60ca31c1"},"source":["# Define an empty tensor to start from\n","A = torch.empty(2, 2)\n","print(A.shape)\n","# 1) Add an extra dimension in axis 0 (unsqueeze)\n","A = A.unsqueeze(0)\n","print(A)\n","print(A.shape)\n","# 2) Add an extra dimension in axis 2\n","A = A.unsqueeze(2)\n","print(A)\n","# 3) Add an extra dimension in axis 2 again\n","A = A.unsqueeze(2)\n","print(A)\n","print('Current A shape after unsqueezing dimensions=(0, 2, 2): ', A.shape)"],"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 2])\n","tensor([[[2.3015e-35, 0.0000e+00],\n","         [3.3631e-44, 0.0000e+00]]])\n","torch.Size([1, 2, 2])\n","tensor([[[[2.3015e-35, 0.0000e+00]],\n","\n","         [[3.3631e-44, 0.0000e+00]]]])\n","tensor([[[[[2.3015e-35, 0.0000e+00]]],\n","\n","\n","         [[[3.3631e-44, 0.0000e+00]]]]])\n","Current A shape after unsqueezing dimensions=(0, 2, 2):  torch.Size([1, 2, 1, 1, 2])\n"]}]},{"cell_type":"code","metadata":{"id":"1YSvBobNz04O","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1663326687261,"user_tz":-120,"elapsed":308,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"74725940-6941-440b-a5b3-c6f5ec25f321"},"source":["# 4) Remove the dimension 0 from step (1)\n","\n","A = A.squeeze(0)\n","\n","print('Current A shape after squeezing dim=0: ', A.shape)"],"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["Current A shape after squeezing dim=0:  torch.Size([2, 1, 1, 2])\n"]}]},{"cell_type":"code","metadata":{"id":"xgf7lhDZz04O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663326710321,"user_tz":-120,"elapsed":268,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"5a69719d-cb84-425e-ed4e-8ac1ad469c9d"},"source":["# 5) We will remove all remaining dimensions of size 1 (\"useless\") when we do not specify the dimension\n","\n","A = A.squeeze()\n","print(A)\n","print('Current A after squeezing all remaining dimensions of size 1: ', A.shape)"],"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[2.3015e-35, 0.0000e+00],\n","        [3.3631e-44, 0.0000e+00]])\n","Current A after squeezing all remaining dimensions of size 1:  torch.Size([2, 2])\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ya3h7RTxT3Nm"},"source":["### Exercise 5\n","\n","Unsqueezing and squeezing dimensions can also be achieved with the [.view()](https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.view) function. \"View\" the tensor *A* to achieve the same shape as the one after step (3) in the previous section with a single function call to *.view()*"]},{"cell_type":"code","metadata":{"id":"sxMSrdduz04P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663326842829,"user_tz":-120,"elapsed":934,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"5b007e3b-6598-4e29-947e-97c5225e8ecc"},"source":["A = torch.empty(2, 2)\n","\n","# TODO: use view to unsqueeze dimensions 0, 2 and 2 (as in the previous section)\n","A = A.view(1, 2, 1, 1, 2)\n","\n","\n","print('Current A shape: ', A.shape)"],"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["Current A shape:  torch.Size([1, 2, 1, 1, 2])\n"]}]},{"cell_type":"markdown","metadata":{"id":"1romtEZvf0j9"},"source":["### Exercise 6 (Broadcasting semantics)\n","\n","Many PyTorch operations support [Broadcasting Semantics](https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics).\n","\n","If a PyTorch operation supports broadcast, then its Tensor arguments can be\n","automatically expanded to be of equal sizes (without making copies of the data).\n","\n","Considering two tensors with at least one dimension, they are \"broadcastable\" if the following conditions are fulfilled, when iterating dimensions jointly from the last one:\n","- Dimensions of both tensors are equal\n","- One of them is 1\n","- One of them does not exist\n","\n","Check these examples:\n","```\n","A.shape = torch.Size([      1])\n","B.shape = torch.Size([3, 1, 7])\n","C = A + B\n","C.shape = torch.Size([3, 1, 7])\n","```\n","\n","```\n","A.shape = torch.Size([5, 1, 4, 1])\n","B.shape = torch.Size([   3, 1, 1])\n","C = A + B\n","C.shape = torch.Size([5, 3, 4, 1])\n","```\n","\n","```\n","A.shape = torch.Size([5, 2, 4, 1])\n","B.shape = torch.Size([   3, 1, 1])\n","C = A + B  # Error, the broadcasting condition is broken in the second dimension\n","```"]},{"cell_type":"code","metadata":{"id":"z4Kwxwirz04Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663327412901,"user_tz":-120,"elapsed":252,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"4fe55c77-d18b-43d7-94f7-086038e4297e"},"source":["a = torch.ones(5, 3, dtype=torch.int16)\n","print(f\"a = {a}\\n\")\n","print(f\"a.shape = {a.shape}\\n\")\n","\n","b = torch.tensor([1, 2, 3])\n","print(f\"b = {b}\\n\")\n","print(f\"b.shape = {b.shape}\\n\")\n","\n","print(f\"a*b = {a*b}\\n\")\n","print(f\"(a*b).shape = {(a*b).shape}\")"],"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["a = tensor([[1, 1, 1],\n","        [1, 1, 1],\n","        [1, 1, 1],\n","        [1, 1, 1],\n","        [1, 1, 1]], dtype=torch.int16)\n","\n","a.shape = torch.Size([5, 3])\n","\n","b = tensor([1, 2, 3])\n","\n","b.shape = torch.Size([3])\n","\n","a*b = tensor([[1, 2, 3],\n","        [1, 2, 3],\n","        [1, 2, 3],\n","        [1, 2, 3],\n","        [1, 2, 3]])\n","\n","(a*b).shape = torch.Size([5, 3])\n"]}]},{"cell_type":"code","metadata":{"id":"6ZVUfAYKz04Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663327462486,"user_tz":-120,"elapsed":6,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"01c594aa-ee4d-42f0-a1ac-b1ccdab5a667"},"source":["a = torch.ones(5, 3, dtype=torch.int16)\n","print(f\"a = {a}\\n\")\n","print(f\"a.shape = {a.shape}\\n\")\n","\n","b = torch.tensor([1, 2, 3, 4, 5])\n","print(f\"b = {b}\\n\")\n","print(f\"b.shape = {b.shape}\\n\")\n","\n","try:\n","    # a and b are not broadcastable, because in the trailing dimension 3 != 5\n","    print(f\"a*b = {a*b}\\n\") # Error\n","except Exception as e:\n","    print(f\"ERROR: {e}\")"],"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["a = tensor([[1, 1, 1],\n","        [1, 1, 1],\n","        [1, 1, 1],\n","        [1, 1, 1],\n","        [1, 1, 1]], dtype=torch.int16)\n","\n","a.shape = torch.Size([5, 3])\n","\n","b = tensor([1, 2, 3, 4, 5])\n","\n","b.shape = torch.Size([5])\n","\n","ERROR: The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 1\n"]}]},{"cell_type":"code","metadata":{"id":"GUi1SZUDz04R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663327602682,"user_tz":-120,"elapsed":257,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"7cea5a6b-2e22-4310-a56d-6cda60458283"},"source":["# TODO: add a trailing dimension to b to make a and b broadcastable\n","b = b.view(5, 1)\n","print(f\"b = {b}\\n\")\n","print(f\"b.shape = {b.shape}\\n\")\n","\n","try:\n","    print(f\"a*b = {a*b}\\n\") # Error\n","except Exception as e:\n","    print(f\"ERROR: {e}\")\n"],"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["b = tensor([[1],\n","        [2],\n","        [3],\n","        [4],\n","        [5]])\n","\n","b.shape = torch.Size([5, 1])\n","\n","a*b = tensor([[1, 1, 1],\n","        [2, 2, 2],\n","        [3, 3, 3],\n","        [4, 4, 4],\n","        [5, 5, 5]])\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"80Zvpx366WlU"},"source":["### Exercise 7 (Grand Finale)\n","\n","Given the tensor *A*, shuffle  each of the elements of the first dimension with the *random.shuffle* Python function. \n","\n","**Clue:** use the functions *torch.chunk*, *random.shuffle* (which acts in-place over Python lists), and *torch.cat*."]},{"cell_type":"code","metadata":{"id":"Jfa_0lCOz04R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663328622253,"user_tz":-120,"elapsed":235,"user":{"displayName":"Gerard Comas Quiles","userId":"07283243575389952521"}},"outputId":"f203aef2-c5b6-4da5-8241-72582d2f8570"},"source":["import torch\n","import random\n","A = torch.rand(4, 2, 4)\n","print('A before shuffling:\\n ', A)\n","\n","# TODO: chunk the tensor, and convert the resulting\n","# tuple into a Python list\n","A  = list(torch.chunk(A, 4, dim=0))\n","\n","\n","# TODO: operate with shuffle over the list\n","random.shuffle(A)\n","\n","# TODO: concatenate the sub-tensors in list \"A\" back\n","# to tensor \"A\"\n","A = torch.cat(A, dim=0)\n","\n","\n","print('A after shuffling:\\n ', A)"],"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["A before shuffling:\n","  tensor([[[0.1195, 0.2407, 0.5835, 0.0221],\n","         [0.0411, 0.1260, 0.2604, 0.0735]],\n","\n","        [[0.9808, 0.0952, 0.7587, 0.0401],\n","         [0.4449, 0.4571, 0.5411, 0.1598]],\n","\n","        [[0.8276, 0.0286, 0.8767, 0.9643],\n","         [0.3105, 0.5286, 0.5375, 0.1580]],\n","\n","        [[0.1522, 0.8428, 0.6576, 0.0021],\n","         [0.1195, 0.7901, 0.2549, 0.5405]]])\n","A after shuffling:\n","  tensor([[[0.8276, 0.0286, 0.8767, 0.9643],\n","         [0.3105, 0.5286, 0.5375, 0.1580]],\n","\n","        [[0.1522, 0.8428, 0.6576, 0.0021],\n","         [0.1195, 0.7901, 0.2549, 0.5405]],\n","\n","        [[0.1195, 0.2407, 0.5835, 0.0221],\n","         [0.0411, 0.1260, 0.2604, 0.0735]],\n","\n","        [[0.9808, 0.0952, 0.7587, 0.0401],\n","         [0.4449, 0.4571, 0.5411, 0.1598]]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"dWqua-bDWatW"},"source":["## Final Words \n","\n","Congrats! You reached the end of this introductory tutorial to PyTorch most fundamental data structure. Managing dimensions, casting dtypes, in-place operations and more are the EVERYDAY to-dos of a deep learner. So get ready to master these before delving into the coolest projects ever in which you'll build deep nets. Being confident with tensor operations is very important to properly design neural networks and avoid bugs!"]},{"cell_type":"markdown","metadata":{"id":"BhDWyIL97LZ9"},"source":["### References\n","\n","[1] https://medium.com/datadriveninvestor/from-scalar-to-tensor-fundamental-mathematics-for-machine-learning-with-intuitive-examples-part-163727dfea8d\n","\n","[2] https://pytorch.org/tutorials/beginner/former_torchies/tensor_tutorial.html\n","\n","[3] https://pytorch.org/docs/stable/tensors.html\n"]}]}